# Enterprise Automation Services Application Assessment
This repository was created to present my submission for the Enterprise Automation Servics Trainee DevOps Engineer technical assessment. It will include all of the code written to fulfill the objectives of the three tasks included in the assessment, as well as this README file which will provide context for the submission and explain some of the design choices made when deciding how to achieve the task objectives. I chose to use Python for this assessment as it is a language I am familiar with which is used widely for data analysis tasks such as this. Python also has a wide variety of well documented modules which can be used to streamline tasks ranging from displaying data in a table in the console to applying natural language processing to a body of text.  
## Task 1: API Consumption and Data Display
### Objective:
Write a script to interact with an API, specifically to retrieve data and display it in a user-friendly format in the console.
### Solution:
The code written for this task can be found in the file _EASTask1.py_ which is included in the repository. The solution is presented in the form of a single function _DisplayUserData()_. The function makes use of two modules imported at the beginning of the script _requests_ and _tabulate_.

The _requests_ module provides functionality for requesting data from an API and in this case is used to import user data from the JSONPlaceholder API using the _.get()_ method in line 11. The JSONPlaceholder API data is organised into 6 common resources which can each be accessed by modifying the URL given in the _requests.get()_, here we are interested in retrieving user data from the API so we add the /users extension to the URL to retrieve only the user data with our request. We store this retrieved data in an object called _response_. When we make a call to the API using _requests.get()_, along with the user data, we retrieve some metadata which we are not interested in for the purposes of this assessment so in line 12 we filter the _response_ data by applying the _.json()_ method to our response object which returns the JSON-encoded content of the response, stored here in _UserData_ as a list of dictionaries. A python dictionary is a data structure designed to allow efficient storage and retrieval of data. Each entry in a dictionary is formed of a key:value pair, values are mapped to keys such that every value in a dictionary has an associated unique key. Keys in python dictionaries immutable, meaning once created they cannot be changed, allowing them to be treated in a way that is more efficient in terms of storage space and access times. This makes dictionaries useful for handling large data sets.]

In line 17 we begin a loop to iterate over each dictionary in the _UserData_ List. Each _User_ dictionary containes two nested dictionaries for the user's address information and information about the user's company. The choice was made to remove these nested dictionaries from the data set, replacing them with entries giving the name of the user's city of residence and their company name. This allows the data to be presented in a table which is, in my opinion, more user-friendly. Since dictionary keys are immutable, the _address_ and _company_ key names cannot be changed. Two new dictionary entries were added to the original dictionary with the city and company names respectively and the _address_ and _company_ entries were removed entirely using the _.pop()_ method. This is all done in lines 18 to 22. Dictionary entries in python cannot be reordered, meaning if we wanted to have, for example, the city name appear in our table where the address would have if it were not removed we would have needed to create an entirely new dictionary. This seemed like a waste of resources for a relatively minor asthetic change which would have little effect on the final output for the task.

Finally we display our data in the console in the form of a _tabulate_ table. The _tabulate_ module allows for easy display of plain text tables using only a single function call, handling column alignment automatically. In line 25 we call the _print()_ function to display text in the console. Within this function we call the _.tbulate()_ function from the _tabulate module_, providing the UserData list of dictionaries as our 2-dimensional data structure to be displayed and specifying that the keys of the dictionaries should be used as the table headers by setting the parameter _headers = 'keys'_. The asthetic choice to use the _'fancy_grid'_ format for our table was made as a personal preference, specified using the _tablefmt_ parameter. The table produced as a result of calling the _DisplayUserData()_ is shown in the GIF below.

![](https://github.com/gethinlewis02/Enterprise-Automation-Services-Application-Assessment/blob/main/EATask1.gif)

## Task 2: Data Storage
### Objective:
Store the retrieved data locally, simulating basic data persistence without the need for a full-fledged database.
### Solution:
The code writtn for task 2 can be found in the file _EASTask2.py_ stored in this repository. The solution function is called _StoreUserData()_. As well as using the _requests_ module discussed in the solution to Task 1 the module _json_ is also required for the solution.

The method for importing data from the API is the same for this task as the one used in Task 1. When considering how best to store the data it was decided that it would be more important for the data to be stored in a machine-readable format and JSON format seemed like the natural choice for this task since it is a versatile format which can easily be handled by python and it is also the format used to by the JSONPlaceholder API meaning data would not need to be converted to an entirely different format between retrieval and storage. The _json_ module which provides functionality for encoding and decoding JSON data was used in line 14 with the _.dumps()_ method to convert the python object _UserData_ into a string in JSON format which can be saved as a text file the optional parameter _indent_ is simply used to specify how much whitespace to apply for an indentation, making the oputput file slightly easier to read.

In line 17 we use the python _open()_ function to specify the file we want to write our data to, specifying the text file _UserData.txt_ in this case. The parameter _'w'_ specifies that we want to overwrite the contents of the _UserData.txt_ file and not simply append our string to the current contents of the file which would be the case if the parameter _'a'_ were specified. finally in line 18 we simply write the contents of the string _JSONObject_ into our text file. The file _UserData.txt_ created as a result of running this function is provided as part of this repository.

## Task 3: Enhanced Data Manipulation
### Objective:
Develop a script that includes capabilities for data transformation and advanced retrieval. 

Devise a function to transform the retrieved data in a meaningful way. This could involve summarizing data, such as categorizing posts, calculating average engagement metrics, or other innovative transformations that add value. 

Store the transformed data in a manner that distinguishes it from the original data, either by using a separate file or a dedicated section within the existing file. Implement functions for more complex data queries, such as filtering for specific user characteristics, searching for keywords within posts, or identifying the top engaging posts.

Showcase your ability to handle and query data sets efficiently and practically.

### Solution:
Three functions were written to manipulate the API data in different ways these are given in _EASTask3.py_. The first function _CountPostComments_ counts the number of comments on each post and displays the post ID alongside the number of comments in a table in the console. The second function _SavePostsByUsersInApartments()_ filters and saves post data corresponding only to posts made by users who have apartments listed in their address line. The third and final function _ListPopularWords()_ uses the _googletrans_ module to translate the body text from every post into englis before applying natural language processing using functionality from the Natural Language Toolkit _nltk_ module, finding the lemmas of each of the words in every post and counting how often each word is used. This data is then stored and displayd in the console in a _tabulate_ table.

#### _CountPostComments()_
The _CountPostComments()_ function was designed to give an impression of how engaging posts in the dataset are based on how many comments are left on a post. This is done by importing post and comment data in a similar way as done in the previous tasks in lines 13 to 17. Next, in line 20 we initialise the list _CommentPostIds_, creating an empty list which will be populated with the ID of the post corresponding to each comment using the _.append()_ method in the following for loop in lines 23 and 24. We then initialise two more empty lists in lines 26 and 27 which are populated with the IDs of every post and the number of comments for each post obtained using the _.count()_ method which counts the number of entries in the CommentPostIds for each post ID.

In line 36 we take our two lists of Post IDs and corresponding comment counts and use the python _zip()_ function which pairs the nth term of two iterable objects together, returning a single 2-dimensional iterator of tuples which can be displayed in a _tabulate_ table which is done in line 39 in the same way as done in task 1 except this time the headers are specified manually.

The list generated by this function is shown in the GIF below, as the data provided by the JSONPlaceholder API is artificial and does not accurately represent real world blog post data we can see that each post has exactly 5 comments, making it difficult to draw any meaningful conclusions about post engagement from this particular dataset. If for example post data were retrieved from a real world social media API such as the one provided by the platform X (formerly known as Twitter), a similar treatment of comment count could be used in conjunction with an analysis of the post text to draw conclusions about what in particular drives engagements with a certain post.

![](https://github.com/gethinlewis02/Enterprise-Automation-Services-Application-Assessment/blob/main/EATask3.gif)

#### _SavePostsByUsersInApartments()_
The _SavePostsByUsersInApartments()_ function showcases a filtering of data retrieved from the API The choice of criteria to filter posts on was somewhat arbitrary but it was decided that as some users addresses listed apartments in the _suite_ line of their address and some listed suites it would provide a way to split the users and show how data could be filtered, changing the filter criteria to something more meaningful with a less artificial dataset would be trivial. 

Post and user data is imported from _JSONPlaceholder_ as usual in lines 47 to 51. We then initialise the _AptUserIds_ list which will be used to store the ID of every user who lives in an apartment we then use a for loop and an if statement in lines 56, 57 and 58 to check if the substring 'Apt.' is present in the _suite_ line of their stored addres, appending their user ID to the _AptUserIds_ list if it is.

In lines 60 to 65 we create an empty list to store the post data for all of the post written by users who live in apartments, we then iterate over all posts, storing the post data in the list if the user ID of the user who created the post is contained in our _AptUserIds_ list.

Lines 67 to 71 then convert the post data to a string in JSON format and store the data in a text file called _AptUserPosts.txt_ in the same way as was done in task 2.

#### _ListPopularWords()_
The _ListPopularWords()_ function demonstrates some basic natural language processing using the natural language toolkit _nltk_ module. Since the post body text in the API data was in the latin language, which is not supported by _nltk_, the _googletrans_ module was used to translate text into english before processing, this also made the output of the function more relevant to an english speaking user.

After importing the post data, the google translator function is initialised as a variable in line 84 so that it can be used later. The _PostBodies_ list is also initialised which will store the translated text of each post. Since the _googletrans_ translate function has a maximum length of text which can be translated, it was nessecary to translate the post body for each post within an iterative loop in lines 90 to 93. This is bad practice and extends the runtime of the file dramatically as the translation operation is reasonably time consuming. It may have been possible to batch a few posts together and iterate the translation fewer times however this method would not have been as versatile and dataset with longer strings in their post bodies may have pushed the size of the batched string over the translator limit, causing an error. The most efficient way to carry out this operation would have been to concatenate all the post bodies before translation and using the official google translate API to translate the text, circumventing the string size limit imposed by the _googletrans_ translator. This, however would have complicated the process of writing this function significantly and it was decided that learning to use this API was best left for another day. The loop in lines 90 to 93 also removes the JSON newline character '\n', replacing it with a simple space so that its presence does not affect the translation.

The strings in the _PostBodies_ list are concatenated in line 95 using the _join()_ function. In lines 98 and 99 we remove punctuation marks which would be tokenized and appear in our output data but would provide no insight and unnessecarily use up computational time.

In line 102 we tokenize our _AllBodyText_ string, splitting the string into each constituent word using the _nltk_ _word_tokenize()_ function. This is the first step towards processing our string to count how often words are used in our post data. We could also tokenize our string into sentences for operations such as sentiment analysis, however with the pseudo-gibberish content of the posts provided by the JSONPlaceholder API it was decided that a simple word frequency analysis would give a more impactful demonstration of natural language processing on this particular dataset.

In line 105 we create a list of 'stopwords', these are words like 'is', 'and' and 'the' which can safely be removed from our tokenized list of words because they provide little to no impactful information and would slow down processing of the text. These words are removed from our tokenized words list in line 108, note here that we have used the _.casefold()_ function as the stopwords list provided by _nltk_ is lowercase and we want to ensure that upper case versions of these stopwords are also removed.

In lines 111 and 112 we convert our filtered and tokenized word list into a list of those words in their lemma form, the form in which that word would appear in an english dictionary. This ensures that when we are counting how many times a word is used, we are also accounting for all of the other forms of that word. We use the _nltk_ _WordNetLemmatizer.lemmatize()_ function for this operation. The function which cannot operate on a list such as ours so we use the python _map()_ function to create an object consisting of each word from the _FilteredBodyWords_ list passed through the _lemmatize()_ function. This method avoids writing an explicit iterative loop which would be less efficient. The output of a _map()_ function is a python _map_ object so we use the _list()_ function to convert our map to a list datatype.

We now create a dictionary in line 115 to store each of our lemmatized words and the number of times that word appears in the _LemmatizedWords_ list. In line 116 we create a new dictionary which has been reordered in descending order of word popularity. The python _sort()_ function is used to achieve this. The _key_ parameter in this function allows the user to specify how the iterable object passed to the function should be sorted, here we have provided a lambda function which specifies that our dictionary should be sorted by its values rather than by its keys, given by the index 1 when each dictionary entry is converted to a list by the _.items()_ modifier. We also use the _reverse = true_ parameter to specify that we want the data in descending order.

Lines 119 to 126 store and print the data as we have done before, saving the data in the text file _WordFrequency.txt_ included in this repository. By looking at our output from this function we can see immediately that the word 'pleasure' occurs most often in our data and the word 'pain' comes second. If our data were from real world blog posts we could, for example, infer from this analysis that, on balance, users found the process of writing their blog posts more pleasurable than painful.

## Final Thoughts
Throughout the process of working on these tasks I have been challenged to work with a lot of new tools and concepts. Coming from an academic background in physics and largely coding self-contained simulations for various projects throughout my studies meant that I had not had a chance to work with tools like an API or natural language processing before this assessment. The process of learning about these tools and how they are used as well as other more basic concepts like python dictionaries has been an extremely interesting experience which I have greatly enjoyed. There were a few things which I would have improved upon given more time, the biggest one being applying a different translation method in the _ListPopularWords()_ function as discussed in that section. I am sure that the function could be sped up dramatically however my own time constraints of working eleven hour night shifts in a bar has limited the amount of time I could committ to this project and getting the function working as intended was my first priority. I would like to thank the EAS team for the opportunity to learn these new skills as part of their assessment process. I am sure that my code could be improved upon and would welcome any feedback on how that could be done. I will happily make myself available to discuss any design choices that do not seem clear from this README.
 
